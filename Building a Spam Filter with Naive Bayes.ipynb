{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "In this project, we're going to build a spam filter for SMS messages using the **Naive Bayes** algorithm. \n",
    "\n",
    "To classify messages as spam or non-spam: \n",
    "- Learns how humans classify messages\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "- Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository]('https://archive.ics.uci.edu/ml/datasets/sms+spam+collection')\n",
    "\n",
    "## 1- Exploring Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection',sep='\\t',header=None,names=['Label','SMS'])\n",
    "print(sms.shape)\n",
    "print(sms['Label'].value_counts(normalize=True)*100)\n",
    "sms.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5572, 2)\n",
      "ham     86.593683\n",
      "spam    13.406317\n",
      "Name: Label, dtype: float64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After checking our dataset, we saw that 87% of the messages are ham (means non-spam) and 13% are spam. Now we're familiar with the dataset, we can start building the spam filter.\n",
    "\n",
    "## 2-Training and Test Set\n",
    "\n",
    "We're going to split our dataset into a training(80% of the dataset) and a test(20% of the dataset) set.\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "# Randomize the dataset\n",
    "random_sms = sms.sample(frac=1,random_state=1)\n",
    "\n",
    "# calculate the index for split\n",
    "training_test_index = round(len(random_sms) * 0.8)\n",
    "\n",
    "# Split the different set\n",
    "training_set = random_sms[:training_test_index].reset_index(drop=True)\n",
    "test_set = random_sms[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print('=====TRAINING======\\n')\n",
    "print(training_set.shape)\n",
    "print(training_set['Label'].value_counts(normalize=True)*100)\n",
    "print('=====TEST======\\n')\n",
    "print(test_set.shape)\n",
    "print(test_set['Label'].value_counts(normalize=True)*100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=====TRAINING======\n",
      "\n",
      "(4458, 2)\n",
      "ham     86.54105\n",
      "spam    13.45895\n",
      "Name: Label, dtype: float64\n",
      "=====TEST======\n",
      "\n",
      "(1114, 2)\n",
      "ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the differents set, we can see the percentages of spam and ham are close to the percentages in the full dataset.\n",
    "\n",
    "## 3- Letter Case and Punctuation\n",
    "\n",
    "To use easily our algorithm we should do some data cleaning to bring the data from this format: \n",
    "\n",
    "![current format](old_format.png)\n",
    "\n",
    "to :\n",
    "\n",
    "![new format](new_format.png)\n",
    "\n",
    "First, let's remove the punctuation and change all letters to lowercase."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "import re\n",
    "# Remove all the punctuation\n",
    "training_set['SMS'] = training_set['SMS'].apply(lambda x: re.sub('\\W',' ',x))\n",
    "# Transform every letter in every word to lower case\n",
    "training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "training_set.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4- Creating the Vocabulary\n",
    "\n",
    "Second, Let's create a vocabulary *(the unique words)* for the messages in the training set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "# Transform each message from the SMS column\n",
    "training_set['SMS'] = training_set['SMS'].str.split()\n",
    "\n",
    "vocabulary = []\n",
    "for words in training_set['SMS']:\n",
    "    for word in words:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary)) \n",
    "print(len(vocabulary))     \n",
    "print(vocabulary[:20])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7783\n",
      "['impressed', 'ryder', '07090201529', 'minmoremobsemspobox45po139wa', 'w1j', 'abt', 'txtstop', 'masteriastering', 'usmle', 'transfer', 'endowed', 'cops', 'stream', 'vl', 'mini', 'prepayment', 'messaged', 'fullonsms', 'withdraw', 'otherwise']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our dictionary, we've **7783** unique words from our training set.\n",
    "\n",
    "## 5- The Final Training Set\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "word_counts_per_sms = {unique_word: [0]*len(training_set['SMS']) \n",
    "for unique_word in vocabulary}\n",
    "\n",
    "for index,sms in enumerate(training_set['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] +=1\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "word_counts.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   impressed  ryder  07090201529  minmoremobsemspobox45po139wa  w1j  abt  \\\n",
       "0          0      0            0                             0    0    0   \n",
       "1          0      0            0                             0    0    0   \n",
       "2          0      0            0                             0    0    0   \n",
       "3          0      0            0                             0    0    0   \n",
       "4          0      0            0                             0    0    0   \n",
       "\n",
       "   txtstop  masteriastering  usmle  transfer  ...  moral  flavour  \\\n",
       "0        0                0      0         0  ...      0        0   \n",
       "1        0                0      0         0  ...      0        0   \n",
       "2        0                0      0         0  ...      0        0   \n",
       "3        0                0      0         0  ...      0        0   \n",
       "4        0                0      0         0  ...      0        0   \n",
       "\n",
       "   09064011000  club4mobiles  gpu  toshiba  difference  unconsciously  doke  \\\n",
       "0            0             0    0        0           0              0     0   \n",
       "1            0             0    0        0           0              0     0   \n",
       "2            0             0    0        0           0              0     0   \n",
       "3            0             0    0        0           0              0     0   \n",
       "4            0             0    0        0           0              0     0   \n",
       "\n",
       "   hahaha  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 7783 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impressed</th>\n",
       "      <th>ryder</th>\n",
       "      <th>07090201529</th>\n",
       "      <th>minmoremobsemspobox45po139wa</th>\n",
       "      <th>w1j</th>\n",
       "      <th>abt</th>\n",
       "      <th>txtstop</th>\n",
       "      <th>masteriastering</th>\n",
       "      <th>usmle</th>\n",
       "      <th>transfer</th>\n",
       "      <th>...</th>\n",
       "      <th>moral</th>\n",
       "      <th>flavour</th>\n",
       "      <th>09064011000</th>\n",
       "      <th>club4mobiles</th>\n",
       "      <th>gpu</th>\n",
       "      <th>toshiba</th>\n",
       "      <th>difference</th>\n",
       "      <th>unconsciously</th>\n",
       "      <th>doke</th>\n",
       "      <th>hahaha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7783 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# Concatenate the training set to the vocabulary \n",
    "training_set_clean = pd.concat([training_set,word_counts],axis=1)\n",
    "training_set_clean.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS  impressed  ryder  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]          0      0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...          0      0   \n",
       "2   ham                    [welp, apparently, he, retired]          0      0   \n",
       "3   ham                                           [havent]          0      0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...          0      0   \n",
       "\n",
       "   07090201529  minmoremobsemspobox45po139wa  w1j  abt  txtstop  \\\n",
       "0            0                             0    0    0        0   \n",
       "1            0                             0    0    0        0   \n",
       "2            0                             0    0    0        0   \n",
       "3            0                             0    0    0        0   \n",
       "4            0                             0    0    0        0   \n",
       "\n",
       "   masteriastering  ...  moral  flavour  09064011000  club4mobiles  gpu  \\\n",
       "0                0  ...      0        0            0             0    0   \n",
       "1                0  ...      0        0            0             0    0   \n",
       "2                0  ...      0        0            0             0    0   \n",
       "3                0  ...      0        0            0             0    0   \n",
       "4                0  ...      0        0            0             0    0   \n",
       "\n",
       "   toshiba  difference  unconsciously  doke  hahaha  \n",
       "0        0           0              0     0       0  \n",
       "1        0           0              0     0       0  \n",
       "2        0           0              0     0       0  \n",
       "3        0           0              0     0       0  \n",
       "4        0           0              0     0       0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>impressed</th>\n",
       "      <th>ryder</th>\n",
       "      <th>07090201529</th>\n",
       "      <th>minmoremobsemspobox45po139wa</th>\n",
       "      <th>w1j</th>\n",
       "      <th>abt</th>\n",
       "      <th>txtstop</th>\n",
       "      <th>masteriastering</th>\n",
       "      <th>...</th>\n",
       "      <th>moral</th>\n",
       "      <th>flavour</th>\n",
       "      <th>09064011000</th>\n",
       "      <th>club4mobiles</th>\n",
       "      <th>gpu</th>\n",
       "      <th>toshiba</th>\n",
       "      <th>difference</th>\n",
       "      <th>unconsciously</th>\n",
       "      <th>doke</th>\n",
       "      <th>hahaha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6- Calculating Constants First\n",
    "\n",
    "We're now done with cleaning the training set, and we can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n",
    "$$\n",
    "P(Spam | w_1,w, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "$$\n",
    "$$\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "$$\n",
    "\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we'll need to use these equations:\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "Some of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:\n",
    "- P(Spam) and P(Ham)\n",
    "- $N_{Spam}$, $N_{Ham}$, $N_{Vocabulary}$\n",
    "\n",
    "We'll also use Laplace smoothing and set $\\alpha = 1$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# Isolate the differents messages\n",
    "spam = training_set_clean[training_set_clean['Label']=='spam']\n",
    "ham = training_set_clean[training_set_clean['Label']=='ham']\n",
    "# Probability of spam message\n",
    "p_spam = (len(spam) / len(training_set_clean) ) * 100\n",
    "# Probability of ham message\n",
    "p_ham = (len(ham) / len(training_set_clean) ) * 100\n",
    "print('====== PROBABILITIES ======')\n",
    "print(p_spam)\n",
    "print(p_ham)\n",
    "print('\\n')\n",
    "\n",
    "number_spam_words= sum(spam['SMS'].apply(len))\n",
    "number_ham_words =  sum(ham['SMS'].apply(len))\n",
    "number_vocabulary_words = len(vocabulary)\n",
    "print(number_spam_words)\n",
    "print(number_ham_words)\n",
    "print(number_vocabulary_words)\n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====== PROBABILITIES ======\n",
      "13.458950201884253\n",
      "86.54104979811575\n",
      "\n",
      "\n",
      "15190\n",
      "57237\n",
      "7783\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7- Calculating Parameters\n",
    "\n",
    "Now that we have the constant terms calculated above, we can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$. Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using the formulas:\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "parameter_spam = {word: 0 for word in vocabulary}\n",
    "parameter_ham = {word: 0 for word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    count_word_spam = spam[word].sum()\n",
    "    count_word_ham = ham[word].sum()\n",
    "    parameter_spam[word] = (count_word_spam + alpha) / (number_spam_words + (alpha * number_vocabulary_words)) \n",
    "    parameter_ham[word] = (count_word_ham + alpha) / (number_ham_words + (alpha * number_vocabulary_words))\n",
    "print(type(count_word_spam))\n",
    "print(type(alpha))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8- Classifying A New Message\n",
    "\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "- Takes in as input a new message (w1, w2, ..., wn)\n",
    "\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "  \n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "def classify(message):\n",
    "    message = re.sub('/W',' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message :\n",
    "        if word in vocabulary:\n",
    "            if word in parameter_ham: p_ham_given_message *= parameter_ham[word]\n",
    "            if word in parameter_spam: p_spam_given_message *= parameter_spam[word]\n",
    "            \n",
    "        \n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')\n",
    "\n",
    "# Test the function\n",
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')\n",
    "classify(\"Sounds good, Tom, then see u there\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "P(Spam|message): 1.0164097981708961e-16\n",
      "P(Ham|message): 1.819563818233026e-17\n",
      "Label: Spam\n",
      "P(Spam|message): 5.359472501724851e-16\n",
      "P(Ham|message): 2.8089018273976986e-12\n",
      "Label: Ham\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9- Measuring the Spam Filter's Accuracy\n",
    "\n",
    "We done creating a spam filter, and classify 2 messages. Now we'll try to determine how well the spam filter does on our test set of 1,114 messages.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# Change the returns of the function \n",
    "def classify_test_set(message):\n",
    "    message = re.sub('/W',' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    for word in message :\n",
    "        if word in vocabulary:\n",
    "            if word in parameter_ham: p_ham_given_message *= parameter_ham[word]\n",
    "            if word in parameter_spam: p_spam_given_message *= parameter_spam[word]\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "\n",
    "# clean the test set \n",
    "test_set['SMS'] = test_set['SMS'].apply(lambda x: re.sub('\\W',' ',x)).str.lower()\n",
    "# create a new column in our test set\n",
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "test_set.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          later i guess  i needa do mcat study too        ham\n",
       "1   ham             but i haf enuff space got like 4 mb          ham\n",
       "2  spam  had your mobile 10 mths  update to latest oran...      spam\n",
       "3   ham  all sounds good  fingers   makes it difficult ...       ham\n",
       "4   ham  all done  all handed in  don t know if mega sh...       ham"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>later i guess  i needa do mcat study too</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>but i haf enuff space got like 4 mb</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>had your mobile 10 mths  update to latest oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>all sounds good  fingers   makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>all done  all handed in  don t know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# Let's calculate the accuracy of the spam filter\n",
    "correct = 0 \n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']: correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "accuracy = (correct / total) * 100\n",
    "print('The accuracy of the spam filter : {}'.format(accuracy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct: 1100\n",
      "Incorrect: 14\n",
      "The accuracy of the spam filter : 98.74326750448833\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seeing the accuracy that reach **98.743 %**, we can conclude our spam filter works well. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,100 correctly.\n",
    "\n",
    "## 10- Why incorrect classification\n",
    "\n",
    "Let's figure out why the algorithm reached the wrong conclusions on the 14 messages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "incorrect = test_set[test_set['predicted'] != test_set['Label']]\n",
    "false_spam = incorrect[(incorrect['predicted']=='spam')&(incorrect['Label']=='ham')].reset_index(drop=True)\n",
    "false_ham = incorrect[(incorrect['predicted']=='ham')&(incorrect['Label']=='spam')].reset_index(drop=True)\n",
    "unclear = incorrect[incorrect['predicted']=='needs human classification'].reset_index(drop=True)\n",
    "\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('UNCLEAR MESSAGES:')\n",
    "for row in unclear.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE HAM MESSAGES:')\n",
    "for row in false_ham.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________\\n')\n",
    "print('FALSE SPAM MESSAGES:')\n",
    "for row in false_spam.iterrows():\n",
    "    print(f'{row[0]+1}. ', row[1]['SMS'])\n",
    "print('_________________________________________________________________________')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________________\n",
      "\n",
      "UNCLEAR MESSAGES:\n",
      "1.  a boy loved a gal  he propsd bt she didnt mind  he gv lv lttrs  bt her frnds threw thm  again d boy decided 2 aproach d gal   dt time a truck was speeding towards d gal  wn it was about 2 hit d girl d boy ran like hell n saved her  she asked  hw cn u run so fast   d boy replied  boost is d secret of my energy  n instantly d girl shouted  our energy  n thy lived happily 2gthr drinking boost evrydy moral of d story   i hv free msgs d    gud ni8\n",
      "_________________________________________________________________________\n",
      "\n",
      "FALSE HAM MESSAGES:\n",
      "1.  not heard from u4 a while  call me now am here all night with just my knickers on  make me beg for it like u did last time 01223585236 xx luv nikiyu4 net\n",
      "2.  more people are dogging in your area now  call 09090204448 and join like minded guys  why not arrange 1 yourself  there s 1 this evening  a 1 50 minapn ls278bb\n",
      "3.  oh my god  i ve found your number again  i m so glad  text me back xafter this msgs cst std ntwk chg  1 50\n",
      "4.  hi babe its chloe  how r u  i was smashed on saturday night  it was great  how was your weekend  u been missing me  sp visionsms com text stop to stop 150p text\n",
      "5.  0a networks allow companies to bill for sms  so they are responsible for their  suppliers   just as a shop has to give a guarantee on what they sell  b  g \n",
      "6.  rct  thnq adrian for u text  rgds vatian\n",
      "7.  2 2 146tf150p\n",
      "8.  hello  we need some posh birds and chaps to user trial prods for champneys  can i put you down  i need your address and dob asap  ta r\n",
      "_________________________________________________________________________\n",
      "\n",
      "FALSE SPAM MESSAGES:\n",
      "1.  unlimited texts  limited minutes \n",
      "2.  26th of july\n",
      "3.  nokia phone is lovly  \n",
      "4.  no calls  messages  missed calls\n",
      "5.  we have sent jd for customer service cum accounts executive to ur mail id  for details contact us\n",
      "_________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Unclear messages: they characterized to be long and have a lot of slang and abbreviation that are not probably in the vocabulary. \n",
    "- False Ham messages: they tend to be rather long and have a high percentage of \"normal\" words, which allows them to override the system.\n",
    "- False Spam messages: they are very short and contain suspicious ad-style words like calls,unlimited,contact us etc. those words are mostly found in spam messages.\n",
    "\n",
    "## 11- Exp : Making the Algorithm sensitive to letter case\n",
    "We're going to update our spam filter by making the algorithm sensitive to letter case and see the result. To do that we are not going to transform the letter to lower case in our training and test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# Our new data set\n",
    "training_set_exp = random_sms[:training_test_index].reset_index(drop=True)\n",
    "test_set_exp = random_sms[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "# Only to Remove the punctuation\n",
    "training_set_exp['SMS'] = training_set_exp['SMS'].apply(lambda x: re.sub('\\W',' ',x))\n",
    "test_set_exp['SMS'] = test_set_exp['SMS'].apply(lambda x: re.sub('\\W',' ',x))\n",
    "# Create the vocabulary\n",
    "vocabulary_exp = []\n",
    "for words in training_set_exp['SMS']:\n",
    "    for word in words:\n",
    "        vocabulary_exp.append(word)\n",
    "vocabulary_exp = list(set(vocabulary_exp))\n",
    "\n",
    "# Count word\n",
    "word_counts_per_sms_exp = {unique_word: [0] * len(training_set_exp['SMS']) for unique_word in vocabulary_exp}\n",
    "for index,sms in enumerate(training_set_exp['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms_exp[word][index] += 1\n",
    "word_counts_exp = pd.DataFrame(word_counts_per_sms_exp)\n",
    "# Concatenate the training set to the vocabulary\n",
    "training_set_final_exp = pd.concat([training_set_exp,word_counts_exp],axis=1)\n",
    "\n",
    "# Isolate the differents messages\n",
    "spam_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='spam']\n",
    "ham_sms_exp = training_set_final_exp[training_set_final_exp['Label']=='ham']\n",
    "# Probability of spam message\n",
    "p_spam_exp = (len(spam_sms_exp) / len(training_set_final_exp) ) * 100\n",
    "# Probability of ham message\n",
    "p_ham_exp = (len(ham_sms_exp) / len(training_set_final_exp) ) * 100\n",
    "\n",
    "number_spam_words= sum(spam_sms_exp['SMS'].apply(len))\n",
    "number_ham_words =  sum(ham_sms_exp['SMS'].apply(len))\n",
    "number_vocabulary_words = len(vocabulary_exp)\n",
    "\n",
    "# Calculation of the parameters\n",
    "parameter_spam = {}\n",
    "parameter_ham = {}\n",
    "\n",
    "for word in vocabulary_exp:\n",
    "    count_word_spam = spam_sms_exp[word].sum()\n",
    "    count_word_ham = ham_sms_exp[word].sum()\n",
    "    parameter_spam[word]= (count_word_spam + alpha) / (number_ham_words + (alpha * number_vocabulary_words))\n",
    "    parameter_ham[word] = (count_word_ham + alpha) / (number_ham_words + (alpha * number_vocabulary_words))\n",
    "#print(vocabulary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# Spam filter \n",
    "def classify_test_set_exp(message):\n",
    "    message = re.sub('/W',' ', message)\n",
    "    message = message.split()\n",
    "    \n",
    "    p_spam_given_message = p_spam_exp\n",
    "    p_ham_given_message = p_ham_exp\n",
    "    for word in message :\n",
    "        if word in vocabulary:\n",
    "            if word in parameter_ham: p_ham_given_message *= parameter_ham[word]\n",
    "            if word in parameter_spam: p_spam_given_message *= parameter_spam[word]\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'\n",
    "\n",
    "# Create a new column in our test set\n",
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "\n",
    "# Let's calculate the accuracy of the spam filter\n",
    "correct = 0 \n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']: correct += 1\n",
    "\n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "accuracy = (correct / total) * 100\n",
    "print('The accuracy of the spam filter : {}'.format(accuracy))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct: 963\n",
      "Incorrect: 151\n",
      "The accuracy of the spam filter : 86.44524236983841\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This experience show us the fact to make the filter sensitive to letter case reduce the accuracy of the filter (the accuracy has dropped by *12.29%*). It seems that the letter case doesn't really make any valuable difference when it comes to distinguishing between spam and ham messages.\n",
    "\n",
    "# Conclusion\n",
    "In this project, we created a spam filter based on the multinomial Naive Bayes algorithmwith with an accuracy of **98.743%**, which is almost 20% higher than our goal (accuracy of 80%)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "data_science",
   "display_name": "data_science",
   "language": "python"
  },
  "interpreter": {
   "hash": "670b0bb49efa43f8d2bcfaa5bac87275253872cc6049e04ea4c254870d9ba8ee"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}